document,questions
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What will the forward pass for a particular op produce?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16","What may not be representable in ""float16""?"
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",Why will the update for the corresponding parameters be lost?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16","What does ""gradient scaling"" do to prevent underflow?"
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What multiplies the network's loss(es) by a scale factor and invokes a backward pass?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",Gradients flowing backward through the network are then scaled by the same factor?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What should be unscaled before the optimizer updates the parameters?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What should not interfere with the learning rate?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",Get_backoff_factor() Returns a Python float containing what?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",get_growth_factor(or grow_int) Returns the growth interval (or growth interval) in what Python int?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What returns a Python int containing the growth interval?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16","What Returns the current scale, or 1.0 if scaling is disabled?"
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What causes a CPU-GPU sync?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What returns a bool indicating whether this instance is enabled?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What is the scaler state?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What is a no-op if this instance is disabled?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What is an object returned from a call to?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What does scale(outputs) Multiplies by?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What returns scaled outputs?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16","If this instance of ""GradScaler"" is not enabled, outputs are returned unmodified?"
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What does set_backoff_factor(new_factor) do?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What is the value to use as the new scale?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What is the value to use as the new scale backoff factor?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What does set_growth_factor do?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What is the value to use as the new growth interval?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What is set_growth_interval?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16","What returns the state of the scaler as a ""dict""?"
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",How many entries does state_dict() contain?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What is the name of the Python float that contains the current scale?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What is the name of the Python float containing the current scale?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What does the current growth factor contain?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What float contains the current backoff factor?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What is a Python int containing the current growth interval?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What is a Python int containing the number of recent consecutive unskipped steps?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16","If this instance is not enabled, returns an empty what?"
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16","What should be called after ""update()""?"
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What should you call if you want to checkpoint the scaler's state after a particular iteration?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16","What does ""update()"" carry out?"
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16","How many operations does ""step()"" perform?"
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16","What operation does ""unscale_(optimizer)"" internal invoke?"
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What was explicitly called for earlier in the iteration?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16","As part of the ""unscale_()"", gradients are checked for what?"
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16","If no inf/NaN gradients are found, what is invoked?"
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What is skipped to avoid corrupting the params?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16","What are forwarded to ""optimizer.step()""?"
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",Returns the return value of what?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What is the name of the Optimizer that applies the gradients?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What are the names of the arguments that can be used?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What does unscale_(optimizer) divide the optimizer's gradient tensors by?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What is optional?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16","What do you need to modify or inspect between the backward pass(s) and ""step()""?"
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16","If ""unscale_()"" is not called explicitly, gradients will be unscaled automatically during what?"
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What is used to enable clipping of unscaled gradients?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16","What does ""unscale_()"" do?"
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What does scaler.scale(loss).backward() do?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What is the name of the tool that updates the scale factor?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What does update(new_scale=None) do?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16","If any optimizer steps were skipped, the scale is multiplied by what to reduce it?"
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16","What does ""growth_factor"" do to increase the scale?"
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",Passing what sets the new scale value manually?
"If the forward pass for a particular op has ""float16"" inputs, the
backward pass for that op will produce ""float16"" gradients. Gradient
values with small magnitudes may not be representable in ""float16",What is used to fill GradScaler's internal scale tensor?
"""torch.amp"" provides convenience methods for mixed precision, where
some operations use the ""torch.float32"" (""float"") datatype and other
operations use lower precision floating point datatype
(""lower_",What provides convenience methods for mixed precision?
"""torch.amp"" provides convenience methods for mixed precision, where
some operations use the ""torch.float32"" (""float"") datatype and other
operations use lower precision floating point datatype
(""lower_","What does ""torch.float32"" mean?"
"""torch.amp"" provides convenience methods for mixed precision, where
some operations use the ""torch.float32"" (""float"") datatype and other
operations use lower precision floating point datatype
(""lower_","How do some ops, like linear layers and convolutions, perform in ""lower_precision_fp"""
"""torch.amp"" provides convenience methods for mixed precision, where
some operations use the ""torch.float32"" (""float"") datatype and other
operations use lower precision floating point datatype
(""lower_","What is often required the dynamic range of ""float32""?"
"""torch.amp"" provides convenience methods for mixed precision, where
some operations use the ""torch.float32"" (""float"") datatype and other
operations use lower precision floating point datatype
(""lower_",Mixed precision tries to match each op to its appropriate what?
"""torch.amp"" provides convenience methods for mixed precision, where
some operations use the ""torch.float32"" (""float"") datatype and other
operations use lower precision floating point datatype
(""lower_","What is the normal mixed precision training with datatype of ""torch.float16"" with?"
"""torch.amp"" provides convenience methods for mixed precision, where
some operations use the ""torch.float32"" (""float"") datatype and other
operations use lower precision floating point datatype
(""lower_",How are Torch.autocast and Turch.cuda.amp.GradScaler modular?
"""torch.amp"" provides convenience methods for mixed precision, where
some operations use the ""torch.float32"" (""float"") datatype and other
operations use lower precision floating point datatype
(""lower_","On what CPU does ""automatic mixed precision training/inference"" on?"
"""torch.amp"" provides convenience methods for mixed precision, where
some operations use the ""torch.float32"" (""float"") datatype and other
operations use lower precision floating point datatype
(""lower_","For CUDA and CPU, APIs are provided separately?"
"""torch.amp"" provides convenience methods for mixed precision, where
some operations use the ""torch.float32"" (""float"") datatype and other
operations use lower precision floating point datatype
(""lower_","What is the equivalent of ""torch.autocast(""cuda"", args...)""?"
"""torch.amp"" provides convenience methods for mixed precision, where
some operations use the ""torch.float32"" (""float"") datatype and other
operations use lower precision floating point datatype
(""lower_","For CPU, only lower precision floating point datatype is supported for now?"
"""torch.amp"" provides convenience methods for mixed precision, where
some operations use the ""torch.float32"" (""float"") datatype and other
operations use lower precision floating point datatype
(""lower_",What type of behavior does CUDA Op-Specific Behavior have?
"""torch.amp"" provides convenience methods for mixed precision, where
some operations use the ""torch.float32"" (""float"") datatype and other
operations use lower precision floating point datatype
(""lower_",What is the default input type for binary_cross_entropy_with_logits?
"""torch.amp"" provides convenience methods for mixed precision, where
some operations use the ""torch.float32"" (""float"") datatype and other
operations use lower precision floating point datatype
(""lower_",What do CPU Op-Specific Behavior do?
"""torch.amp"" provides convenience methods for mixed precision, where
some operations use the ""torch.float32"" (""float"") datatype and other
operations use lower precision floating point datatype
(""lower_",What can CPU Ops that can autocast to?
"""torch.amp"" provides convenience methods for mixed precision, where
some operations use the ""torch.float32"" (""float"") datatype and other
operations use lower precision floating point datatype
(""lower_",* CPU Ops that promote to the widest input type?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","What do instances of ""autocast"" serve as?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What are context managers or decorators?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What is chosen by autocast to improve performance while maintaining accuracy?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","When entering an autocast-enabled region, what type of Tensors may be?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What should you not call on your model(s) or inputs when using autocast?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","What should ""autocast"" wrap only the forward pass(s) of your network?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What are not recommended under autocast?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What type of ops run in the same type that autocast used for corresponding forward ops?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",# Creates model and optimizer in default precision model?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What does optimizer do for input?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What is the target in data?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",# Enables autocasting for the forward pass (model + loss) with what function?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","What does loss = loss_fn(output, target) do?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",Exits the context manager before what?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What does optimizer.step() do?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What can be used as a CUDA example?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What can also be used as a decorator?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","What can be used on the ""forward"" method of your model?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","@autocast() def forward(self, input): Floating-point Tensors produced in an autocast-enabled region may be what?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What may an autocast-enabled region be?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What may cause type mismatch errors?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","When casting the Tensor(s) produced in the autocast region back to ""float32""?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What does autocast region back to if desired?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What is cast a no-op and incurs no additional overhead?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",# Creates some tensors in default dtype (here assumed to be float32)?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","a_float32 = torch.rand((8, 8), device=""cuda"") what is b_float 32 = torch?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What is on autocast's list of ops that should run in float16?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What should run in float16?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","Inputs are float32, but the op runs in what?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What does f_float16 mean?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","What does torch.mm(a_float32, b_float 32) handle?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","What does torch.mm(d_float32, e_float16) do after exiting autocast?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What is the name of the program that creates model and optimizer in default precision?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What is the default precision model?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",what is the target in data: for input?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","For input, target in data: optimizer.zero_grad() # Runs the forward pass with what?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","What does torch.autocast(device_type=""cpu"", dtype=torch.bfloat16) do?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What is loss.backward() optimizer.step()?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What does optimizer.step() CPU Inference Example: # Creates model in default precision model = Net().eval()?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",# Runs the forward pass with what?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",output = model(input) CPU Inference Example
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","What is output = model(input) CPU Inference Example with Jit Trace: class TestModel(nn.Module): def __init__(self, input_size, num_classes): super(TestModel, self)?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","What is super(TestModel, self).__init__() self.fc1 = nn.Linear(input_size, num_classes)?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","What does return self.fc1(x) input_size = 2 num_classes = 2 model = TestModel(input_size, num clases)?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","For now, we suggest to disable what?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","# For now, we suggest to disable what?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",https://github.com/pytorch/issues/75956 torch?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","What is model = torch.jit.trace(model, torch.randn(1, input_size))?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",Models Run for what type of mismatch?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What are type mismatch errors in an autocast-enabled region?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",How can subregions be nested in autocast enabled regions?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","What can be useful if you want to force a subregion to run in a particular ""dtype""?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What gives you explicit control over the execution type?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What should inputs from the surrounding region be cast to before use?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",# Creates some tensors in default dtype (here assumed to be what?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","a_float32 = torch.rand((8, 8), device=""cuda"") what is a float32?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",b_float 32 is what?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",with autocast(): what is the name of the device?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What does e_float16 do?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What does autocast(enabled=False) call to ensure float32 execution?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",# Calls e_float16.float() to ensure float32 execution?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",# No manual casts are required when re-entering the autocast-enabled region?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",# torch.mm runs in float16 and produces what output?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What is the autocast state?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What must be invoked if you want it enabled in a new thread?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What must be invoked in a new thread?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","What affects ""torch.nn.DataParallel"" when used with more than one GPU per process?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What should be enabled in the device?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What should autocasting be enabled?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What should be enabled in the region?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What is the default?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What is the default value of the class torch.cuda.amp.autocast?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What does cache_enabled do?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What is a helper decorator for forward methods of custom autograd functions?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",See the example page for more detail.
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","default=None) -- If ""forward"" runs in an autocast-enabled region, what casts incoming floating-point CUDA Tensors to the target dtype?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","What happens if the decorated ""forward"" is called outside of an autocast-enabled ops?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","What is a no-op if the decorated ""forward"" is called outside of an autocast-enabled region?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed",What is the helper decorator for backward methods of custom autograd functions?
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","""backward"" executes with the same autocast state as what?"
"class torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None)

   Instances of ""autocast"" serve as context managers or decorators
   that allow regions of your script to run in mixed","What is equivalent to ""torch.cpu.amp.autocast(""cpu"", args...)?"""
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac",What types of ops are not eligible?
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac",Out-of-place ops and Tensor methods are what?
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac",In-place variants and calls that explicitly supply what are allowed in autocast-enabled regions?
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac",What can autocast in an autocast-enabled region?
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac",What can't autocast?
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac","For best performance and stability, prefer out-of-place ops in what regions?"
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac",Ops called with an explicit what are not eligible?
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac","What does eligible produce that respects the ""dtype"" argument?"
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac",What describes the behavior of eligible ops in autocast- enabled regions?
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac",What happens if functions are exposed in multiple namespaces?
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac",Ops not listed below do not go through what?
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac",What may still change the type in which unlisted ops run?
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac",Which unlisted ops run if they're downstream from autocasted ops?
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac","If an op is unlisted, we assume it's numerically stable in what?"
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac","""GRUCell"", ""linear"", ""LSTMCell,"" ""mm"", ""mv"", ""prelu"", ""RNNCell""?"
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac","""binary_cross_entropy_with_logits"", ""acos"", ""cdist"", ""cosine_similarity"", ""cross_enttropy"", ""cumprod,"" ""cumsum"", ""dist"" and ""erfinv"" are examples of what?"
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac",What do these ops take multiple inputs and require that the inputs' dtypes match?
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac","What is the op run in if all inputs are ""float16""?"
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac","What does autocast cast all inputs to if any of the inputs is ""float32""?"
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac",Which ops natively promote inputs without autocasting's intervention?
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac","What ops not listed here (e.g., binary ops like ""add"") do?"
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac","What does autocasting do when inputs are a mixture of ""float16"" and ""float32""?"
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac","What does the backward passes of ""torch.nn.functional.binary_cross_entropy()"" produce?"
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac","In autocast-enabled regions, the forward input may be what?"
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac","What does ""binary_cross_entropy"" and ""BCELoss"" raise an error in?"
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac",Many models use a sigmoid layer right before what?
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac",What is safe to autocast?
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac",What do ops always go through?
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac",What are ops invoked as part of a function or as a method?
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac",By what means do unlisted ops run?
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac",What type of ops may be changed if they're downstream from autocasted ops?
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac","If you believe an unlisted op is numerically unstable in ""bfloat16"", please file an issue what?"
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac",What do these ops don't require a particular?
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac",What do these ops not require for stability?
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac",What does autocast cast all inputs to?
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac",What do bfloat16 and float32 ops run in?
"Ops that run in ""float64"" or non-floating-point dtypes are not
eligible, and will run in these types whether or not autocast is
enabled.

Only out-of-place ops and Tensor methods are eligible. In-plac","Run in ""float32"" and produce what output?"
